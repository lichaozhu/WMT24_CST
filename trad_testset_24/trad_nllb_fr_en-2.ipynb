{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7439be6-a321-4a74-8ae4-b234675699e9",
   "metadata": {},
   "source": [
    "### translate with baseline (nllb 3.3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10211b-f203-41d1-8e6a-504e92199bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a419c4-2d50-457d-86df-ad7fde86643b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "\n",
    "import accelerate\n",
    "from accelerate.utils import get_balanced_memory\n",
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import tensor_parallel as tp\n",
    "\n",
    "#print(torch.cuda.is_available())\n",
    "#for i in range(torch.cuda.device_count()):\n",
    " #  print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "src_lang = \"fra_Latn\"\n",
    "tgt_lang = \"eng_Latn\"\n",
    "\n",
    "model_id = \"facebook/nllb-200-3.3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, load_in_8bit=False, torch_dtype=torch.float16, src_lang=src_lang)\n",
    "#balanced_low_0 installs the model into cuda1 (any GPU but cuda0)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=False, device_map=\"balanced_low_0\")\n",
    "\n",
    "#accelerate.disk_offload(model,offload_dir=\"/tmp/lichao\")\n",
    "#print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb317e-0b31-4b80-85a8-0ac2d24d3977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df340ad9-38be-4a58-be2c-af45a0811cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = open('../wmt_data_STC/test2022.33B.trad.en','w',encoding=\"utf8\")\n",
    "\n",
    "\n",
    "ef = pd.read_csv('testset_stc_24/STC-testset-en-fr.csv')\n",
    "ef_en = pd.concat([ef.loc[ef['source_language'] == 'en']['source'], ef.loc[ef['target_language'] == 'en']['reference']],ignore_index=True)\n",
    "ef_fr = pd.concat([ef.loc[ef['target_language'] == 'fr']['reference'], ef.loc[ef['source_language'] == 'fr']['source']],ignore_index=True)\n",
    "\n",
    "lines = list(ef_en)\n",
    "\n",
    "\n",
    "for line in lines :\n",
    "    #inputs = tokenizer(line, return_tensors=\"pt\")\n",
    "    inputs = tokenizer(line, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "#    print(inputs)\n",
    "    #forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang]\n",
    "    #tokenizer.convert_tokens_to_ids(\"deu_Latn\")\n",
    "    #tokenizer.convert_tokens_to_ids(\"deu_Latn\")\n",
    "    \n",
    "    translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30)\n",
    "    print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])\n",
    "    a = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    new_file.write(str(a))\n",
    "    new_file.write('\\n')\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a3037-1fd5-4fed-a6e2-fbfc01fc9e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('MT_perf/test.csv')\n",
    "\n",
    "new_data = open(\"MT_perf/test2022_test_nllb33B.txt\", \"w+\", encoding=\"utf8\")\n",
    "# Function to translate texts based on their source language\n",
    "def translate_text(text, src_lang, tgt_lang):\n",
    "    translator = GoogleTranslator(source=src_lang, target=tgt_lang)\n",
    "    print(translator.translate(text))\n",
    "    return translator.translate(text)\n",
    "\n",
    "\n",
    "# Translate texts based on source_language\n",
    "\n",
    "#for index, row in data.iterrows():\n",
    " #   if row['source_language'] == 'fr':\n",
    "  #      translated_text = translate_text(row['source'], 'fr', 'en')\n",
    "   #     new_data.write(translated_text)\n",
    "    #    new_data.write('\\n')\n",
    "    #elif row['source_language'] == 'en':\n",
    "     #   translated_text = translate_text(row['source'], 'en', 'fr')\n",
    "      #  new_data.write(translated_text)\n",
    "       # new_data.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    inputs = tokenizer(row['source'], return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30)\n",
    "    print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])\n",
    "    a = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    new_data.write(str(a))\n",
    "    new_data.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31c883-c5a4-4555-813e-ebecd91d24a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('MT_perf/en-fr.csv')\n",
    "\n",
    "new_data = open(\"MT_perf/valid_2024_nllb33B_fr_en.txt\", \"w+\", encoding=\"utf8\")\n",
    "# Function to translate texts based on their source language\n",
    "def translate_text(text, src_lang, tgt_lang):\n",
    "    translator = GoogleTranslator(source=src_lang, target=tgt_lang)\n",
    "    print(translator.translate(text))\n",
    "    return translator.translate(text)\n",
    "\n",
    "\n",
    "# Translate texts based on source_language\n",
    "\n",
    "#for index, row in data.iterrows():\n",
    " #   if row['source_language'] == 'fr':\n",
    "  #      translated_text = translate_text(row['source'], 'fr', 'en')\n",
    "   #     new_data.write(translated_text)\n",
    "    #    new_data.write('\\n')\n",
    "    #elif row['source_language'] == 'en':\n",
    "     #   translated_text = translate_text(row['source'], 'en', 'fr')\n",
    "      #  new_data.write(translated_text)\n",
    "       # new_data.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['source_language'] == 'fr':\n",
    "    #inputs = tokenizer(line, return_tensors=\"pt\")\n",
    "        inputs = tokenizer(row['source'], return_tensors=\"pt\").to(\"cuda:1\")\n",
    "        translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"), max_length=30)\n",
    "        print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])\n",
    "        a = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        new_data.write(str(a))\n",
    "        new_data.write('\\n')\n",
    "    elif row['source_language'] == 'en':\n",
    "        inputs = tokenizer(row['source'], return_tensors=\"pt\").to(\"cuda:1\")\n",
    "        translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30)\n",
    "        print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])\n",
    "        a = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        new_data.write(str(a))\n",
    "        new_data.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94bc3c-d888-45f1-9eff-a8ec68e9e7a6",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71230235-7b69-4fef-a09a-326bb2bdb2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034c8c5-ddd2-45ae-9178-db3eb16de0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dffcf0-044a-4e21-8733-84c2dd081cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b4490-a883-48fd-b239-bc9bb17c3219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8b9c4-8bfd-449c-b7ec-885baa7e11fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eeb31b-f91b-47dc-8b28-80b521014d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83316894-4df7-4801-a320-752d58655dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f4c0a-7670-4364-9a04-626ae61a2174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, NllbTokenizer\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "from accelerate.utils import get_balanced_memory\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "#import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset, DatasetDict\n",
    "import datasets\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-3.3B\", src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\")\n",
    "\n",
    "train_parquet = pd.read_parquet('test.parquet')\n",
    "\n",
    "\n",
    "train_parquet = Dataset.from_pandas(train_parquet)\n",
    "\n",
    "raw_datasets = datasets.DatasetDict({\"train\":train_parquet})\n",
    "\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.95, seed=20)\n",
    "\n",
    "\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "print(split_datasets)\n",
    "\n",
    "max_length = 128\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    #print(model_inputs)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20623e7e-6eff-472c-ba60-fc5d64f26776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, NllbTokenizer\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "from accelerate.utils import get_balanced_memory\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#import os\n",
    "##os.environ[\"NCCL_SHM_DISABLE\"]=\"1\"\n",
    "#torch.cuda.empty_cache()\n",
    "#import gc\n",
    "##del variables\n",
    "#gc.collect()\n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-3.3B\", load_in_8bit=False, device_map=\"balanced_low_0\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-3.3B\", load_in_8bit=False, device_map=\"balanced_low_0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "\n",
    "#from transformers import whoami\n",
    "\n",
    "#user = whoami(token=\"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\")\n",
    "\n",
    "#dataset = load_dataset('csv', data_files={'train': \"train.csv\",'validation':\"validation.csv\",'test': \"test.csv\"})\n",
    "\n",
    "#train_data = dataset[\"train\"]\n",
    "#val_data = dataset[\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "#def tokenize_function(example):\n",
    "#    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "#tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "#metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"ft_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    #test_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
